# Quick Reference: Regenerating Dashboard Data

## âš¡ Quick Commands

### Regenerate All Years (Recommended)
```bash
cd /home/yannn/projects/Yannngn/sinasc-dashboard/sinasc_research
python src/pipeline/05_create_dashboard_data.py --all
```

### Regenerate Single Year
```bash
python src/pipeline/05_create_dashboard_data.py --year 2024
```

### Custom Directories
```bash
python src/pipeline/05_create_dashboard_data.py --all \
  --data_dir data/SINASC \
  --output_dir dashboard_data \
  --input_name engineered_features.parquet
```

---

## ðŸ“‹ What Gets Regenerated

### New Metrics Added

#### 1. **Monthly Aggregates** (`monthly_YYYY.parquet`)
- âœ¨ `cesarean_count`: Absolute number of cesarean births per month

#### 2. **State Aggregates** (`state_YYYY.parquet`)
- âœ¨ `cesarean_count`: Absolute number of cesarean births by state
- âœ¨ `adolescent_pregnancy_pct`: % of births to mothers < 20 years
- âœ¨ `adolescent_pregnancy_count`: Count of adolescent pregnancies
- âœ¨ `very_young_pregnancy_pct`: % of births to mothers < 15 years
- âœ¨ `very_young_pregnancy_count`: Count of very young pregnancies

---

## ðŸŽ¯ Why Regenerate?

The Geographic Dashboard page requires:
1. **Complete state-level metrics** for regional comparisons
2. **Adolescent pregnancy data** for demographic analysis
3. **Consistent count/percentage pairs** for dual visualizations

The Annual Analysis page now uses:
1. **Cesarean counts** for absolute number charts
2. **Cesarean percentages** for rate trend charts

---

## â±ï¸ Expected Runtime

| Years | Records | Time | Disk Space |
|-------|---------|------|------------|
| 1     | ~3M     | ~30s | ~50 MB     |
| 7     | ~20M    | ~4m  | ~350 MB    |

*Tested on: Intel i5 / 16GB RAM / SSD*

---

## ðŸ“¦ Output Structure

```
dashboard_data/
â”œâ”€â”€ aggregates/
â”‚   â”œâ”€â”€ monthly_2018.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2019.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2020.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2021.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2022.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2023.parquet          â† Updated
â”‚   â”œâ”€â”€ monthly_2024.parquet          â† Updated
â”‚   â”œâ”€â”€ state_2018.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2019.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2020.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2021.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2022.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2023.parquet            â† Updated
â”‚   â”œâ”€â”€ state_2024.parquet            â† Updated
â”‚   â”œâ”€â”€ municipality_2018.parquet     â† No changes
â”‚   â”œâ”€â”€ municipality_2019.parquet     â† No changes
â”‚   â””â”€â”€ ...
â”œâ”€â”€ years/
â”‚   â””â”€â”€ ...                            â† No changes
â””â”€â”€ metadata.json                      â† Updated (includes new years)
```

---

## âœ… Verification

After regeneration, verify the new columns exist:

```python
import pandas as pd

# Check monthly aggregates
monthly = pd.read_parquet('dashboard_data/aggregates/monthly_2024.parquet')
assert 'cesarean_count' in monthly.columns, "Missing cesarean_count in monthly"
print("âœ… Monthly aggregates OK")

# Check state aggregates
state = pd.read_parquet('dashboard_data/aggregates/state_2024.parquet')
required = [
    'cesarean_count',
    'adolescent_pregnancy_pct',
    'adolescent_pregnancy_count',
    'very_young_pregnancy_pct',
    'very_young_pregnancy_count'
]
missing = [col for col in required if col not in state.columns]
assert len(missing) == 0, f"Missing columns: {missing}"
print("âœ… State aggregates OK")

print("\nðŸŽ‰ All new metrics present!")
```

---

## ðŸ”„ Before Regenerating

### 1. Backup Current Data (Optional)
```bash
# Create backup
cp -r dashboard_data dashboard_data_backup_$(date +%Y%m%d)

# Or move to backup folder
mv dashboard_data dashboard_data_old
```

### 2. Check Available Source Data
```bash
# List available years in SINASC data directory
ls -d data/SINASC/*/

# Expected output:
# data/SINASC/2018/
# data/SINASC/2019/
# data/SINASC/2020/
# data/SINASC/2021/
# data/SINASC/2022/
# data/SINASC/2023/
# data/SINASC/2024/
```

### 3. Verify Source Files Exist
```bash
# Check that engineered_features.parquet exists for each year
for year in {2018..2024}; do
  file="data/SINASC/$year/engineered_features.parquet"
  if [ -f "$file" ]; then
    size=$(du -h "$file" | cut -f1)
    echo "âœ… $year: $size"
  else
    echo "âŒ $year: MISSING"
  fi
done
```

---

## ðŸš¨ Troubleshooting

### Error: "File not found"
```
âš ï¸  File not found: data/SINASC/2024/engineered_features.parquet
```

**Solution**: Run the feature engineering pipeline first:
```bash
python src/pipeline/03_create_engineered_features.py --year 2024
```

### Error: "Memory error"
**Solution**: Process years individually instead of `--all`:
```bash
for year in {2018..2024}; do
  python src/pipeline/05_create_dashboard_data.py --year $year
done
```

### Error: "Column not found"
**Solution**: Ensure you're using the latest pipeline code:
```bash
git pull origin dev
```

---

## ðŸ“Š Expected Log Output

```
============================================================
Processing Year: 2024
============================================================

ðŸ“¥ Loading data from data/SINASC/2024/engineered_features.parquet...
  âœ… Loaded 2,834,567 records with 51 columns

ðŸ“… Creating monthly aggregates for 2024...
  âœ… Saved 12 months of aggregates (0.02 MB)

ðŸ—ºï¸  Creating state aggregates for 2024...
  âœ… Saved 27 states with aggregates (0.01 MB)

ðŸ™ï¸  Creating municipality aggregates for 2024 (top 500)...
  âœ… Saved 500 municipalities with aggregates (0.03 MB)

ðŸ“ˆ Creating yearly summary for 2024...
  âœ… Created yearly summary

âœ… Year 2024 processing complete!

============================================================
Creating Combined Files
============================================================

ðŸ“Š Creating yearly aggregates...
  âœ… Saved yearly aggregates (0.01 MB)

ðŸ“ˆ Creating combined yearly data...
  âœ… Combined data from 7 years (0.05 MB)

ðŸ“ Creating metadata...
  âœ… Saved metadata (0.01 MB)

============================================================
Dashboard Data Creation Complete!
============================================================

ðŸ“¦ Output directory: dashboard_data
ðŸ“Š Total size: 350.2 MB
ðŸ“… Years processed: 7
ðŸ“ Total records: 19,841,489

âœ¨ Ready for dashboard deployment!
```

---

## ðŸŽ¯ Next Steps After Regeneration

1. **Restart Dashboard** (if running):
   ```bash
   # Stop current instance
   pkill -f "python dashboard/app.py"
   
   # Start fresh
   python dashboard/app.py
   ```

2. **Test Geographic Page**:
   - Navigate to http://localhost:8050/geographic
   - Select different years in dropdown
   - Verify all indicators load correctly
   - Check regional comparison chart
   - Verify state ranking table

3. **Test Annual Page**:
   - Navigate to http://localhost:8050/annual
   - Check "NÃºmero Mensal de CesÃ¡reas" chart loads
   - Verify YoY badges appear on metric cards
   - Confirm all charts render without errors

4. **Verify Data Quality**:
   ```bash
   python -c "
   import pandas as pd
   df = pd.read_parquet('dashboard_data/aggregates/state_2024.parquet')
   print('State columns:', len(df.columns))
   print('Has cesarean_count:', 'cesarean_count' in df.columns)
   print('Has adolescent metrics:', 'adolescent_pregnancy_pct' in df.columns)
   "
   ```

---

## ðŸ“š Related Documentation

- **Pipeline Enhancement Details**: `docs/PIPELINE_GEOGRAPHIC_ENHANCEMENT.md`
- **Geographic Page Implementation**: `docs/GEOGRAPHIC_IMPLEMENTATION.md`
- **YoY Change Feature**: `docs/YOY_CHANGE_FEATURE.md`
- **IBGE Data Guide**: `data/IBGE/README.md`

---

*Quick reference guide - Keep this handy for future regenerations!*
